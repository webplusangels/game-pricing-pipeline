import requests
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from pathlib import Path
from requests.exceptions import HTTPError, Timeout, ConnectionError
from datetime import datetime
from config import settings

from util.io_helper import save_csv, load_csv, upload_to_s3, download_from_s3
from util.cache_manager import CacheManager
from util.logger import setup_logger
from util.rate_limit_manager import RateLimitManager

class ITADIdFetcher:
    def __init__(self, 
                 output_dir="./data/raw", 
                 cache_dir="./data/cache",
                 error_dir="./data/error",
                 log_dir='./log/fetcher',
                 max_retries=3, 
                 thread_workers=2):
        # ------------------------- ì„¤ì • -------------------------
        self.MAX_RETRIES = max_retries
        self.THREAD_WORKERS = thread_workers
        self.OUTPUT_DIR = Path(output_dir)
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR = Path(cache_dir)
        self.CACHE_DIR.mkdir(exist_ok=True)
        self.ERROR_DIR = Path(error_dir)
        self.ERROR_DIR.mkdir(exist_ok=True)
        self.LOG_DIR = Path(log_dir)
        self.LOG_DIR.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = setup_logger(
            name="itad_id_fetcher", 
            log_dir=self.LOG_DIR,
        )

        # ìš”ì²­ ì œì–´ê¸° ì„¤ì •
        self.rate_limit_manager = RateLimitManager()
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.CACHE_FILE = self.CACHE_DIR / "itad_id_status_cache.json"
        self.FAILED_IDS_FILE = self.ERROR_DIR / "itad_failed_ids.csv"
        self.OUTPUT_FILE = self.OUTPUT_DIR / "itad_game_ids.csv"
        
        # API ì„¤ì •
        self.ID_BASE_URL = "https://api.isthereanydeal.com/games/lookup/v1"
        self.KEY = settings.ITAD_KEY
                
        # S3 ì—…ë¡œë“œ ì„¤ì •
        self.S3_CACHE_KEY = "data/cache/itad_id_status_cache.json"
        self.S3_OUTPUT_KEY = "data/raw/itad_game_ids.csv"
        if self.CACHE_FILE.exists():
            self.logger.info("ğŸ“ ë¡œì»¬ ìºì‹œ ì‚¬ìš©")
        elif download_from_s3(self.S3_CACHE_KEY, self.CACHE_FILE):
            self.logger.info("âœ… S3 ìºì‹œ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
        else:
            self.logger.warning("â— ìºì‹œ íŒŒì¼ ì—†ìŒ. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")

        # í—¤ë” ì„¤ì •
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        # ------------------------- ë°ì´í„° ì €ì¥ì†Œ -------------------------
        self.fetched_data = []
        self.failed_list = []
        self.errored_list = []
        self.cache = CacheManager(self.CACHE_FILE)
        
    def fetch_game_info(self, row):
        """ITAD APIì—ì„œ ê²Œì„ ID ê°€ì ¸ì˜¤ê¸°"""
        app_id = row['appid']
        name = row.get('name', 'Unknown')
        cached = self.cache.get(app_id)
        
        if cached and cached.get("status") == "success":
            return False
        if self.cache.too_many_fails(app_id):
            self.logger.info(f"ğŸš« ì•± {app_id}ì€ ì‹¤íŒ¨ê°€ ëˆ„ì ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        url = f"{self.ID_BASE_URL}?key={self.KEY}&appid={app_id}"
        
        try:
            response = requests.get(url, headers=self.HEADERS, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            # ì‘ë‹µ ë°ì´í„° í™•ì¸
            if data is None:
                self.failed_list.append(app_id)
                self.cache.record_fail(app_id)
                self.logger.warning(f"[{app_id}] ë¹ˆ ì‘ë‹µ")
                return False
                
            if "found" not in data:
                self.failed_list.append(app_id)
                self.cache.record_fail(app_id)
                self.logger.warning(f"[{app_id}] 'found' í‚¤ê°€ ì—†ìŒ: {data}")
                return False
                
            # ë°ì´í„° ì²˜ë¦¬
            if data["found"] == True and "game" in data and "id" in data["game"]:
                itad_id = data["game"]["id"]
                processed_data = {
                    "appid": app_id,
                    "name": name,
                    "itad_id": itad_id,
                    "collected_at": datetime.now().isoformat()
                }
                
                self.fetched_data.append(processed_data)
                self.cache.set(app_id, {
                    "status": "success",
                })
                self.logger.info(f"[{app_id}] ì„±ê³µì ìœ¼ë¡œ ITAD ID ì°¾ìŒ: {itad_id}")
                return True
            else:
                # foundê°€ Falseì¸ ê²½ìš° ë˜ëŠ” game/id í‚¤ê°€ ì—†ëŠ” ê²½ìš°
                processed_data = {
                    "appid": app_id,
                    "name": name,
                    "itad_id": None,
                    "collected_at": datetime.now().isoformat()
                }
                
                self.fetched_data.append(processed_data)
                self.cache.record_fail(app_id)
                self.logger.info(f"[{app_id}] ê²Œì„ì„ ITADì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
                
        except Timeout:
            self.errored_list.append(app_id)
            self.cache.record_fail(app_id)
            self.logger.warning(f"[{app_id}] ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            raise
            
        except ConnectionError as e:
            self.errored_list.append(app_id)
            self.cache.record_fail(app_id)
            self.logger.error(f"[{app_id}] ì—°ê²° ì˜¤ë¥˜: {e}")
            raise
            
        except HTTPError as e:
            self.errored_list.append(app_id)
            status_code = getattr(e.response, 'status_code', None)
            self.cache.set(app_id, f"http_error_{status_code}")
            
            if status_code == 429:  # Rate limit
                self.logger.warning(f"[{app_id}] ìš”ì²­ ì œí•œ ê°ì§€")
                self.rate_limit_manager.handle_rate_limit(app_id)
            raise
                
        except Exception as e:
            self.errored_list.append(app_id)
            self.cache.record_fail(app_id)
            self.logger.error(f"[{app_id}] ì—ëŸ¬ ë°œìƒ: {str(e)}")
            raise
    
    def save_checkpoint(self):
        """í˜„ì¬ ìˆ˜ì§‘ ìƒíƒœ ì €ì¥"""
        try:
            # id ë°ì´í„° ì €ì¥
            new_data_df = pd.DataFrame(self.fetched_data)
            
            # ê¸°ì¡´ CSV ë¡œë“œ ë° ë³‘í•©
            if self.OUTPUT_FILE.exists():
                old_data_df = load_csv(self.OUTPUT_FILE)
                merged_df = pd.concat([old_data_df, new_data_df], ignore_index=True)
                merged_df.drop_duplicates(subset="appid", inplace=True)
            else:
                merged_df = new_data_df
            
            # ì €ì¥
            save_csv(merged_df, self.OUTPUT_FILE)
            self.cache.save()       

            # ì €ì¥ í›„ ë°ì´í„° ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            self.fetched_data = []
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def fetch_in_parallel(self, app_ids, batch_size=40):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì²˜ë¦¬"""
        base_delay = 0.5
        
        for i in range(0, len(app_ids), batch_size):
            batch = app_ids[i:i+batch_size]
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{i+len(batch)}/{len(app_ids)}")
            
            # ìš”ì²­ ì œí•œ ìƒí™©ì— ë”°ë¼ ì§€ì—° ì‹œê°„ ì¡°ì •
            request_delay = self.rate_limit_manager.get_current_delay(base_delay)
            if self.rate_limit_manager.should_slow_down():
                self.logger.info(f"âš ï¸ ìš”ì²­ ì œí•œ ê°ì§€ë¡œ ì§€ì—° ì‹œê°„ ì¦ê°€: {request_delay:.2f}ì´ˆ")
            
            with ThreadPoolExecutor(max_workers=self.THREAD_WORKERS) as executor:
                # ìš”ì²­ ì œì¶œ ì‹œ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€
                futures = []
                for app_id in batch:
                    futures.append((app_id, executor.submit(self.fetch_game_info, app_id)))

                for app_id, future in tqdm(futures, desc="ğŸ“¦ Fetching"):
                    try:
                        should_sleep = future.result()
                        if should_sleep:
                            time.sleep(base_delay)
                    except Exception as e:
                        self.logger.error(f"ìŠ¤ë ˆë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê° ë°°ì¹˜ í›„ ì €ì¥
            self.save_checkpoint()
    
    def retry_loop(self, target_list, label, input_df, max_retries=3):
        """ì‹¤íŒ¨í•œ ìš”ì²­ ì¬ì‹œë„"""
        for i in range(max_retries):
            if not target_list:
                break
                
            self.logger.info(f"ğŸ” {label} ì¬ì‹œë„ {i+1}íšŒ - ëŒ€ìƒ {len(target_list)}ê°œ")
            
            retry_targets = list(set(target_list.copy()))
            target_list.clear()
            
            # ì¬ì‹œë„í•  í–‰ ì¶”ì¶œ
            retry_rows = input_df[input_df['appid'].isin(retry_targets)].to_dict('records')
            
            # ì¬ì‹œë„ëŠ” ë” ì ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬
            self.fetch_in_parallel(retry_rows, batch_size=20)
            time.sleep(2)  # ì¬ì‹œë„ ì‚¬ì´ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
    
    def get_collected_appids(self):
        """ì´ë¯¸ ìˆ˜ì§‘ëœ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        collected_appids = set()
        if self.OUTPUT_FILE.exists():
            try:
                existing_df = load_csv(self.OUTPUT_FILE)
                collected_appids.update(existing_df["appid"].tolist())
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return collected_appids
    
    def run(self, input_csv_path, id_column="appid"):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            # ì…ë ¥ ë°ì´í„°ì—ì„œ ID ëª©ë¡ ë¡œë“œ
            df_input = load_csv(input_csv_path)
            
            # í•„ìˆ˜ ì¹¼ëŸ¼ í™•ì¸
            if id_column not in df_input.columns:
                raise ValueError(f"ì…ë ¥ CSV íŒŒì¼ì— '{id_column}' ì¹¼ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # appid ì¹¼ëŸ¼ì´ ì—†ìœ¼ë©´ id_columnìœ¼ë¡œ ë³µì‚¬
            if 'appid' not in df_input.columns and id_column != 'appid':
                df_input['appid'] = df_input[id_column]
            
            # name ì¹¼ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if 'name' not in df_input.columns:
                df_input['name'] = 'Unknown'
            
            # ì´ë¯¸ ìˆ˜ì§‘ëœ ID ì œì™¸
            collected_ids = self.get_collected_appids()
            df_to_collect = df_input[~df_input['appid'].isin(collected_ids)]

            # ìˆ˜ì§‘ ëŒ€ìƒ appid ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            rows_to_collect = df_to_collect.to_dict("records")
            
            # 1ì°¨ ìˆ˜ì§‘
            self.fetch_in_parallel(rows_to_collect)
            
            # ì‹¤íŒ¨í•œ ID ê°€ì ¸ì˜¤ê¸°
            failed_ids_from_cache = [
                int(app_id) for app_id, status in self.cache.items()
                if status == "failed"
            ]
            self.logger.info(f"ìºì‹œì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_cache)}")
    
            # ì‹¤íŒ¨í•œ ID íŒŒì¼ì—ì„œ ì¶”ê°€ ì‹¤íŒ¨ ID ê°€ì ¸ì˜¤ê¸°
            if self.FAILED_IDS_FILE.exists():
                failed_ids_from_file = load_csv(self.FAILED_IDS_FILE)["appid"].tolist()
            else:
                failed_ids_from_file = []
            self.logger.info(f"íŒŒì¼ì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_file)}")
            
            # ì‹¤íŒ¨í•œ ID í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
            retry_ids = set(failed_ids_from_cache + failed_ids_from_file)
            
            # ì´ë¯¸ ì„±ê³µí•œ IDëŠ” ì œì™¸
            retry_ids = [appid for appid in retry_ids if self.cache.get(appid) != "success"]
            retry_ids = list(set(retry_ids))
            self.logger.info(f"ì¬ì‹œë„í•  ID ìˆ˜: {len(retry_ids)}")
                
            # ì¬ì‹œë„ ë£¨í”„
            retry_stages = [
                {"label": "ì—ëŸ¬", "targets": self.errored_list, "max_retries": 3},
                {"label": "ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 2},
                {"label": "ìµœì¢… ì—ëŸ¬", "targets": self.errored_list, "max_retries": 1},
                {"label": "ìµœì¢… ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 1},
                {"label": "ë§ˆì§€ë§‰", "targets": retry_ids, "max_retries": 1}
            ]
            
            for stage in retry_stages:
                target_list = stage["targets"]
                label = stage["label"]
                max_retries = stage["max_retries"]
                
                # ì¤‘ë³µ ì œê±°
                target_list[:] = list(set(target_list))
                
                if target_list:
                    self.retry_loop(target_list, label, df_input, max_retries)
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            self.logger.info("\nâœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ì´ ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
            if self.OUTPUT_FILE.exists():
                final_df = load_csv(self.OUTPUT_FILE)
                found_count = final_df['itad_id'].notna().sum()
                self.logger.info(f"ğŸ¯ ì´ ìˆ˜ì§‘ëœ ê²Œì„ ìˆ˜: {len(final_df)}")
                self.logger.info(f"ğŸ” ITADì—ì„œ ì°¾ì€ ê²Œì„ ìˆ˜: {found_count} (ì „ì²´ ì¤‘ {found_count/len(final_df)*100:.1f}%)")
            
            self.logger.info(f"âŒ ìµœì¢… ì‹¤íŒ¨: {len(self.failed_list)}ê°œ, ì—ëŸ¬: {len(self.errored_list)}ê°œ")
            
            # ì‹¤íŒ¨í•œ ID ëª©ë¡ ì €ì¥
            if self.failed_list or self.errored_list:
                failed_df = pd.DataFrame({
                    "appid": self.failed_list + self.errored_list,
                    "status": ["failed"] * len(self.failed_list) + ["error"] * len(self.errored_list)
                })
                save_csv(failed_df, self.ERROR_DIR / "itad_failed_ids.csv")
                self.logger.info(f"â— ì‹¤íŒ¨í•œ ID ëª©ë¡ì´ itad_failed_ids.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            # S3 ì—…ë¡œë“œ
            upload_to_s3(self.OUTPUT_FILE, self.S3_OUTPUT_KEY)
            upload_to_s3(self.CACHE_FILE, self.S3_CACHE_KEY)
            self.logger.info(f"âœ… S3ì— ì—…ë¡œë“œ ì™„ë£Œ: {self.S3_OUTPUT_KEY}, {self.S3_CACHE_KEY}")
            
        except Exception as e:
            self.logger.error(f"í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

# # ì‚¬ìš© ì˜ˆì‹œ
# if __name__ == "__main__":
#     fetcher = ITADIdFetcher()
#     fetcher.run(settings.PAID_LIST_DIR)
