import requests
import time
from datetime import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path
from requests.exceptions import HTTPError, Timeout, ConnectionError

from util.io_helper import load_json, save_json, save_csv  
from util.logger import setup_logger
from util.retry import retry_on_exception

class SteamReviewFetcher:
    def __init__(self, 
                 output_dir="./data/raw", 
                 cache_dir="./data/cache",
                 error_dir="./data/error",
                 log_dir='./log/fetcher',
                 max_retries=3, 
                 thread_workers=2):
        # ------------------------- ì„¤ì • -------------------------
        self.MAX_RETRIES = max_retries
        self.THREAD_WORKERS = thread_workers
        self.OUTPUT_DIR = Path(output_dir)
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR = Path(cache_dir)
        self.CACHE_DIR.mkdir(exist_ok=True)
        self.ERROR_DIR = Path(error_dir)
        self.ERROR_DIR.mkdir(exist_ok=True)
        self.LOG_DIR = Path(log_dir)
        self.LOG_DIR.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = setup_logger(
            name="steam_review_fetcher", 
            log_dir=self.LOG_DIR,
        )
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.CACHE_FILE = self.CACHE_DIR / "review_status_cache.json"
        self.FAILED_IDS_FILE = self.ERROR_DIR / "failed_review_ids.csv"
        self.reviews_df_path = self.OUTPUT_DIR / "steam_game_reviews.csv"
        
        # í—¤ë” ì„¤ì •
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        # ------------------------- ë°ì´í„° ì €ì¥ì†Œ -------------------------
        self.reviews_data = []
        self.failed_list = []
        self.errored_list = []
        self.status_cache = self._load_cache()
        
    def _load_cache(self):
        """ìºì‹œ íŒŒì¼ì—ì„œ ìƒíƒœ ì •ë³´ ë¡œë“œ"""
        try:
            cache = load_json(self.CACHE_FILE)
            if not isinstance(cache, dict):
                self.logger.warning("âš ï¸ ìºì‹œ íŒŒì¼ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                return {}
            # ì¤‘ë³µ í‚¤ ì œê±° (ë§ˆì§€ë§‰ ê°’ ìœ ì§€)
            deduplicated_cache = {}
            for key, value in cache.items():
                deduplicated_cache[key] = value
            
            # ë¡œê¹… ì¶”ê°€: ì¤‘ë³µ ì œê±°ëœ í‚¤ì˜ ìˆ˜ í™•ì¸
            original_count = len(cache)
            deduplicated_count = len(deduplicated_cache)
            
            if original_count != deduplicated_count:
                self.logger.info(f"ğŸ” ìºì‹œì—ì„œ {original_count - deduplicated_count}ê°œì˜ ì¤‘ë³µ í‚¤ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return deduplicated_cache
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            return {}
        
    @retry_on_exception(max_retries=2, 
                    exceptions=(Timeout, ConnectionError, HTTPError), 
                    logger=None)
    def fetch_review_data(self, app_id):
        """Steam APIì—ì„œ ë¦¬ë·° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if app_id in self.status_cache and self.status_cache[app_id] == "success":
            return
        
        url = f"https://store.steampowered.com/appreviews/{app_id}?json=1&filter=all&language=all&day_range=all&review_type=all&purchase_type=all"
        
        try:
            response = requests.get(url, headers=self.HEADERS, timeout=10)
            response.raise_for_status()
            response_json = response.json()

            if response_json.get("success") != 1:
                self.failed_list.append(app_id)
                self.status_cache[app_id] = "failed"
                return
            
            query_summary = response_json.get("query_summary", {})
            if query_summary:
                review_data = {
                    "appid": app_id,
                    "num_reviews": query_summary.get("num_reviews", 0),
                    "review_score": query_summary.get("review_score", 0),
                    "total_reviews": query_summary.get("total_reviews", 0)
                }
                self.reviews_data.append(review_data)
                self.status_cache[app_id] = {
                    "status": "success",
                    "collected_at": datetime.now().isoformat()
                }
            else:
                self.failed_list.append(app_id)
                self.status_cache[app_id] = "no_data"
                
        except Timeout:
            self.errored_list.append(app_id)
            self.status_cache[app_id] = "timeout"
            self.logger.warning(f"[{app_id}] ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            raise
            
        except ConnectionError as e:
            self.errored_list.append(app_id)
            self.status_cache[app_id] = "connection_error"
            self.logger.error(f"[{app_id}] ì—°ê²° ì˜¤ë¥˜: {e}")
            raise
            
        except HTTPError as e:
            self.errored_list.append(app_id)
            status_code = getattr(e.response, 'status_code', None)
            self.status_cache[app_id] = f"http_error_{status_code}"
            
            if status_code == 429:  # Rate limit
                self.logger.warning(f"[{app_id}] ìš”ì²­ ì œí•œ ê°ì§€")
            raise
                
        except Exception as e:
            self.errored_list.append(app_id)
            self.status_cache[app_id] = "error"
            self.logger.error(f"[{app_id}] ì—ëŸ¬ ë°œìƒ: {e}")
            raise
        
    def save_checkpoint(self):
        """í˜„ì¬ ìˆ˜ì§‘ ìƒíƒœ ì €ì¥"""
        try:
            # ë¦¬ë·° ë°ì´í„° ì €ì¥
            new_reviews_df = pd.DataFrame(self.reviews_data)
            
            # ê¸°ì¡´ CSV ë¡œë“œ ë° ë³‘í•©
            if self.reviews_df_path.exists():
                old_reviews_df = pd.read_csv(self.reviews_df_path)
                merged_reviews_df = pd.concat([old_reviews_df, new_reviews_df], ignore_index=True)
                merged_reviews_df.drop_duplicates(subset="appid", inplace=True)
            else:
                merged_reviews_df = new_reviews_df
                
            # ìƒíƒœ ìºì‹œì˜ ì¤‘ë³µ ì œê±°
            clean_status_cache = {}
            for key, value in self.status_cache.items():
                clean_status_cache[key] = value
            
            # ì €ì¥ (io_helperì˜ save_csv, save_json í™œìš©)
            save_csv(merged_reviews_df, self.reviews_df_path)
            save_json(self.CACHE_FILE, clean_status_cache)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            total = len(self.reviews_data) + len(self.failed_list) + len(self.errored_list)
            success_rate = len(self.reviews_data) / total * 100 if total > 0 else 0
            self.logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ - ëˆ„ì  ìˆ˜ì§‘: {len(merged_reviews_df)}ê°œ")
            self.logger.info(f"ì§„í–‰ ìƒí™©: {len(self.reviews_data)}ê°œ ì„±ê³µ / {len(self.failed_list)}ê°œ ì‹¤íŒ¨ / {len(self.errored_list)}ê°œ ì˜¤ë¥˜ (ì„±ê³µë¥ : {success_rate:.1f}%)")
            
            # ì €ì¥ í›„ ë°ì´í„° ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            self.reviews_data = []
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def fetch_in_parallel(self, app_ids, batch_size=40):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì²˜ë¦¬"""
        for i in range(0, len(app_ids), batch_size):
            batch = app_ids[i:i+batch_size]
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{i+len(batch)}/{len(app_ids)}")
            
            with ThreadPoolExecutor(max_workers=self.THREAD_WORKERS) as executor:
                # ìš”ì²­ ì œì¶œ ì‹œ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€
                futures = []
                for app_id in batch:
                    futures.append(executor.submit(self.fetch_review_data, app_id))
                    time.sleep(0.8)  # ìš”ì²­ ê°„ 800ms ì§€ì—°
                
                for future in tqdm(as_completed(futures), total=len(batch), desc="ğŸ“¦ Fetching"):
                    try:
                        future.result()
                    except Exception as e:
                        self.logger.error(f"ìŠ¤ë ˆë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê° ë°°ì¹˜ í›„ ì €ì¥
            self.save_checkpoint()
    
    def retry_loop(self, target_list, label, max_retries=3):
        """ì‹¤íŒ¨í•œ ìš”ì²­ ì¬ì‹œë„"""
        for i in range(max_retries):
            if not target_list:
                break
                
            self.logger.info(f"ğŸ” {label} ì¬ì‹œë„ {i+1}íšŒ - ëŒ€ìƒ {len(target_list)}ê°œ")
            
            retry_targets = list(set(target_list))
            target_list.clear()
            
            # ì¬ì‹œë„ëŠ” ë” ì ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬
            self.fetch_in_parallel(retry_targets, batch_size=20)
            time.sleep(2)  # ì¬ì‹œë„ ì‚¬ì´ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
    
    def get_collected_appids(self):
        """ì´ë¯¸ ìˆ˜ì§‘ëœ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        collected_appids = set()
        if self.reviews_df_path.exists():
            try:
                existing_df = pd.read_csv(self.reviews_df_path)
                collected_appids.update(existing_df["appid"].tolist())
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return collected_appids
    
    def run(self, input_csv_path):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        # ì´ë¯¸ ìˆ˜ì§‘ëœ ID í™•ì¸
        collected_appids = self.get_collected_appids()
        
        # ID ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ë° í•„í„°ë§
        ids_list = pd.read_csv(input_csv_path)["appid"].tolist()
        ids_list = [appid for appid in ids_list if appid not in collected_appids]
        self.logger.info(f"ğŸ“‹ ìƒˆë¡œ ìˆ˜ì§‘í•  appid ìˆ˜: {len(ids_list)}")
        
        # 1ì°¨ ìˆ˜ì§‘
        self.fetch_in_parallel(ids_list)
        
        # ì‹¤íŒ¨í•œ ID ê°€ì ¸ì˜¤ê¸°
        failed_ids_from_cache = [
            int(app_id) for app_id, status in self.status_cache.items()
            if status == "failed"
        ]
        self.logger.info(f"ìºì‹œì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_cache)}")

        # ì‹¤íŒ¨í•œ ID íŒŒì¼ì—ì„œ ì¶”ê°€ ì‹¤íŒ¨ ID ê°€ì ¸ì˜¤ê¸°
        if self.FAILED_IDS_FILE.exists():
            failed_ids_from_file = pd.read_csv(self.FAILED_IDS_FILE)["id"].tolist()
        else:
            failed_ids_from_file = []
        self.logger.info(f"íŒŒì¼ì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_file)}")
        
        # ì‹¤íŒ¨í•œ ID í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
        retry_ids = set(failed_ids_from_cache + failed_ids_from_file)
        
        # ì´ë¯¸ ì„±ê³µí•œ IDëŠ” ì œì™¸
        retry_ids = [appid for appid in retry_ids if self.status_cache.get(appid) != "success"]
        retry_ids = list(set(retry_ids))
        self.logger.info(f"ì¬ì‹œë„í•  ID ìˆ˜: {len(retry_ids)}")
        
        # ì¬ì‹œë„ ë£¨í”„
        retry_stages = [
            {"label": "ì—ëŸ¬", "targets": self.errored_list, "max_retries": 3},
            {"label": "ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 2},
            {"label": "ìµœì¢… ì—ëŸ¬", "targets": self.errored_list, "max_retries": 1},
            {"label": "ìµœì¢… ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 1},
            {"label": "ë§ˆì§€ë§‰", "targets": retry_ids, "max_retries": 1},
        ]
        
        for stage in retry_stages:
            target_list = stage["targets"]
            label = stage["label"]
            max_retries = stage["max_retries"]
            
            # ì¤‘ë³µ ì œê±°
            target_list[:] = list(set(target_list))
            
            if target_list:
                self.retry_loop(target_list, label, max_retries)
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        self.logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì´ ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
        if self.reviews_df_path.exists():
            final_df = pd.read_csv(self.reviews_df_path)
            self.logger.info(f"ğŸ¯ ì´ ìˆ˜ì§‘ëœ ê²Œì„ ë¦¬ë·° ìˆ˜: {len(final_df)}")
        
        self.logger.info(f"âŒ ìµœì¢… ì‹¤íŒ¨: {len(self.failed_list)}ê°œ, ì—ëŸ¬: {len(self.errored_list)}ê°œ")
        
        # ì‹¤íŒ¨í•œ ID ëª©ë¡ ì €ì¥
        if self.failed_list or self.errored_list:
            failed_df = pd.DataFrame({
                "id": self.failed_list + self.errored_list,
                "status": ["failed"] * len(self.failed_list) + ["error"] * len(self.errored_list)
            })
            save_csv(failed_df, self.FAILED_IDS_FILE)
            self.logger.info(f"â— ì‹¤íŒ¨í•œ ID ëª©ë¡ì´ {self.FAILED_IDS_FILE}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
