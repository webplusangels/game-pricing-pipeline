from config import settings
from sqlalchemy import create_engine
from pathlib import Path
import os

from util.io_helper import load_csv
from util.logger import setup_logger

class DBUploader:
    def __init__(self):
        self.engine = create_engine(settings.DB_URL)
        self.data_dir = Path("data/processed")
        
        self.logger = setup_logger(
            name="DBUploader",
            log_dir="log/db_uploader",
        )

    def run(self, dry_run=False):
        from sqlalchemy import MetaData
        metadata = MetaData()
        
        # í…Œì´ë¸” ì´ë¦„ê³¼ ì¸ë±ìŠ¤ ì»¬ëŸ¼ ë§¤í•‘
        index_columns_map = {
            "category": ["id"],
            "platform": ["id"],
            "game_static": ["id"],
            "game_dynamic": ["game_id"],
            "game_category": ["id"],
            "current_price_by_platform": ["game_id", "platform_id"]
        }
        
        try:
            with self.engine.begin() as conn:
                for file_path in self.data_dir.glob("*_removed.csv"):
                    table = file_path.stem.replace("_removed", "")
                    df = load_csv(file_path)
                    if dry_run:
                        self.logger.info(f"[Dry Run] {table}ì—ì„œ {len(df)}ê°œ í–‰ì´ ì‚­ì œë  ì˜ˆì •ì…ë‹ˆë‹¤.")
                        continue
                    
                    # self.delete_rows(table, df)
                    self.delete_rows(table, df, conn)
                    self.logger.info(f"ğŸ—‘ï¸ {table}ì—ì„œ {len(df)}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ")
                    if file_path.exists():
                        os.remove(file_path)
                        
                for file_path in self.data_dir.glob("*_updated.csv"):
                    table = file_path.stem.replace("_updated", "")
                    df = load_csv(file_path)

                    if dry_run:
                        self.logger.info(f"[Dry Run] {table}ì— {len(df)}ê°œ í–‰ì´ ì—…ë¡œë“œ(ë˜ëŠ” ìˆ˜ì •)ë  ì˜ˆì •ì…ë‹ˆë‹¤.")
                        continue
                    
                    # self.insert_or_update_data(table, df, file_path, index_columns)
                    self.insert_or_update_data(table, df, file_path, index_columns_map[table], conn)
                    self.logger.info(f"âœ… {table}ì— {len(df)}ê°œ í–‰ ì—…ë¡œë“œ(ë˜ëŠ” ìˆ˜ì •) ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"DB ì—°ê²° ì˜¤ë¥˜ í˜¹ì€ íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            raise e
        
        # for file_path in self.data_dir.glob("*_removed.csv"):
        #     table = file_path.stem.replace("_removed", "")
        #     df = load_csv(file_path)
        #     if dry_run:
        #         self.logger.info(f"[Dry Run] {table}ì—ì„œ {len(df)}ê°œ í–‰ì´ ì‚­ì œë  ì˜ˆì •ì…ë‹ˆë‹¤.")
        #         continue
            
        #     # self.delete_rows(table, df)
        #     with self.engine.begin() as conn:
        #         self.delete_rows(table, df, conn)
        #     self.logger.info(f"ğŸ—‘ï¸ {table}ì—ì„œ {len(df)}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ")
        #     if file_path.exists():
        #         os.remove(file_path)
            
        # for file_path in self.data_dir.glob("*_updated.csv"):
        #     table = file_path.stem.replace("_updated", "")
        #     df = load_csv(file_path)

        #     if dry_run:
        #         self.logger.info(f"[Dry Run] {table}ì— {len(df)}ê°œ í–‰ì´ ì—…ë¡œë“œ(ë˜ëŠ” ìˆ˜ì •)ë  ì˜ˆì •ì…ë‹ˆë‹¤.")
        #         continue
            
        #     index_columns_map = {
        #         "category": ["id"],
        #         "platform": ["id"],
        #         "game_static": ["id"],
        #         "game_dynamic": ["game_id"],
        #         "game_category": ["id"],
        #         "current_price_by_platform": ["game_id", "platform_id"]
        #     }

        #     # self.insert_or_update_data(table, df, file_path, index_columns_map[table])
        #     with self.engine.begin() as conn:
        #         self.insert_or_update_data(table, df, file_path, index_columns_map[table], conn)
        #     self.logger.info(f"âœ… {table}ì— {len(df)}ê°œ í–‰ ì—…ë¡œë“œ(ë˜ëŠ” ìˆ˜ì •) ì™„ë£Œ")
    
    def delete_rows(self, table_name, df, conn):
        from sqlalchemy import Table, MetaData, tuple_

        index_columns_map = {
            "category": ["id"],
            "platform": ["id"],
            "game_static": ["id"],
            "game_dynamic": ["game_id"],
            "game_category": ["id"],
            "current_price_by_platform": ["game_id", "platform_id"]
        }

        index_columns = index_columns_map.get(table_name)
        if not index_columns:
            self.logger.error(f"âŒ {table_name}ì— ëŒ€í•œ ì‚­ì œ í‚¤ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        metadata = MetaData()
        table = Table(table_name, metadata, autoload_with=self.engine)

        CHUNK_SIZE = 100
        
        # with self.engine.begin() as conn:
        #     keys = [tuple(row[col] for col in index_columns) for _, row in df.iterrows()]
        #     for i in range(0, len(keys), CHUNK_SIZE):
        #         chunk = keys[i:i + CHUNK_SIZE]
        #         self.logger.info(f"ğŸ—‘ï¸ {table_name}ì—ì„œ {len(chunk)}ê°œ í–‰ ì‚­ì œ ì¤‘...")
        #         conn.execute(
        #             table.delete().where(
        #                 tuple_(*[table.c[col] for col in index_columns]).in_(chunk)
        #             )
        #         )
        
        keys = [tuple(int(row[col]) for col in index_columns) for _, row in df.iterrows()]
        for i in range(0, len(keys), CHUNK_SIZE):
            chunk = keys[i:i + CHUNK_SIZE]
            self.logger.info(f"ğŸ—‘ï¸ {table_name}ì—ì„œ {len(chunk)}ê°œ í–‰ ì‚­ì œ ì¤‘...")
            conn.execute(
                table.delete().where(
                    tuple_(*[table.c[col] for col in index_columns]).in_(chunk)
                )
            )
  
    def insert_or_update_data(self, table_name, df, file_path, index_columns, conn):
        from sqlalchemy import Table, MetaData
        from sqlalchemy.dialects.postgresql import insert as pg_insert

        df = df.drop(columns=[c for c in ["deleted_at", "updated_at", "created_at"] if c in df.columns], errors="ignore")
        df = df.astype(object).where(df.notnull(), None)
        
        metadata = MetaData()
        table = Table(table_name, metadata, autoload_with=self.engine)

        CHUNK_SIZE = 100

        # with self.engine.begin() as conn:
        #     for i in range(0, len(df), CHUNK_SIZE):
        #         chunk_df = df.iloc[i:i + CHUNK_SIZE]
        #         values = chunk_df.to_dict(orient="records")
                
        #         insert_stmt = pg_insert(table).values(values)
        #         update_cols = {c: insert_stmt.excluded[c] for c in chunk_df.columns if c not in index_columns}
        #         stmt = insert_stmt.on_conflict_do_update(
        #             index_elements=index_columns,
        #             set_=update_cols,
        #         )
        #         self.logger.info(f"{table_name}ì— {len(values)}ê°œ í–‰ ì‚½ì…(ë˜ëŠ” ìˆ˜ì •) ì¤‘...")
        #         conn.execute(stmt)
        
        for i in range(0, len(df), CHUNK_SIZE):
            chunk_df = df.iloc[i:i + CHUNK_SIZE]
            values = chunk_df.to_dict(orient="records")
            
            insert_stmt = pg_insert(table).values(values)
            update_cols = {c: insert_stmt.excluded[c] for c in chunk_df.columns if c not in index_columns}
            stmt = insert_stmt.on_conflict_do_update(
                index_elements=index_columns,
                set_=update_cols,
            )
            self.logger.info(f"{table_name}ì— {len(values)}ê°œ í–‰ ì‚½ì…(ë˜ëŠ” ìˆ˜ì •) ì¤‘...")
            conn.execute(stmt)

        if file_path.exists():
            os.remove(file_path)
            