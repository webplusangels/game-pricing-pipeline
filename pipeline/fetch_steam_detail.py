import requests
import time
import html
from datetime import datetime
import pandas as pd
import boto3
from tqdm import tqdm
from pathlib import Path
from requests.exceptions import HTTPError, Timeout, ConnectionError

from util.io_helper import save_csv, load_csv, upload_to_s3, download_from_s3
from util.cache_manager import CacheManager
from util.logger import setup_logger
from util.rate_limit_manager import RateLimitManager

class SteamDetailFetcher:
    def __init__(self, 
                 output_dir="./data/raw", 
                 cache_dir="./data/cache",
                 error_dir="./data/error",
                 log_dir='./log/fetcher',
                 max_retries=3, 
                 thread_workers=2):
        # ------------------------- ì„¤ì • -------------------------
        self.MAX_RETRIES = max_retries
        self.THREAD_WORKERS = thread_workers
        self.OUTPUT_DIR = Path(output_dir)
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR = Path(cache_dir)
        self.CACHE_DIR.mkdir(exist_ok=True)
        self.ERROR_DIR = Path(error_dir)
        self.ERROR_DIR.mkdir(exist_ok=True)
        self.LOG_DIR = Path(log_dir)
        self.LOG_DIR.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = setup_logger(
            name="steam_detail_fetcher", 
            log_dir=self.LOG_DIR,
        )
        
        # ìš”ì²­ ì œì–´ê¸° ì„¤ì •
        self.rate_limit_manager = RateLimitManager()
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.CACHE_FILE = self.CACHE_DIR / "detail_status_cache.json"
        self.FAILED_IDS_FILE = self.ERROR_DIR / "failed_detail_ids.csv"
        self.original_df_path = self.OUTPUT_DIR / "steam_game_detail_original.csv"
        self.parsed_df_path = self.OUTPUT_DIR / "steam_game_detail_parsed.csv"
        
        # S3 ì—…ë¡œë“œ ì„¤ì •
        self.S3_CACHE_KEY = "data/cache/detail_status_cache.json"
        self.S3_OUTPUT_ORIGINAL_KEY = "data/raw/steam_game_detail_original.csv"
        self.S3_OUTPUT_PARSED_KEY = "data/raw/steam_game_detail_parsed.csv"
        
        if self.CACHE_FILE.exists():
            self.logger.info("ğŸ“ ë¡œì»¬ ìºì‹œ ì‚¬ìš©")
        elif download_from_s3(self.S3_CACHE_KEY, self.CACHE_FILE):
            self.logger.info("âœ… S3 ìºì‹œ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
        else:
            self.logger.warning("â— ìºì‹œ íŒŒì¼ ì—†ìŒ. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")

        # í—¤ë” ì„¤ì •
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        # ------------------------- ë°ì´í„° ì €ì¥ì†Œ -------------------------
        self.original_data = {}
        self.parsed_data = []
        self.failed_list = []
        self.errored_list = []
        self.cache = CacheManager(self.CACHE_FILE)
        
    def clean_html_entities(self, text):
        """
        html ìš”ì†Œì™€ zwnbsp ì œê±°
        """
        if pd.isna(text):
            return text
        return html.unescape(text.replace('\uFEFF', ''))   
    
    def upload_image_to_s3(self, image_url, app_id):
        """
        ì´ë¯¸ì§€ë¥¼ S3ì— ì—…ë¡œë“œí•˜ê³  URL ë°˜í™˜
        """
        
        bucket_name = "steam-image-store"
        key = f'images/{app_id}.jpg'
        s3_url = f"https://{bucket_name}.s3.ap-northeast-2.amazonaws.com/images/{key}"
        
        try:
            image_data = requests.get(image_url, timeout=10).content
            s3 = boto3.client('s3')
            s3.put_object(
                Bucket=bucket_name,
                Key=key,
                Body=image_data,
                ContentType='image/jpeg',
                ACL='public-read'
            )
            return s3_url
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {app_id} / {e}")
            return None
        
    def parse_game_data(self, data):
        """API ì‘ë‹µì—ì„œ í•„ìš”í•œ ê²Œì„ ì •ë³´ ì¶”ì¶œ"""
        if not isinstance(data, dict) or not data.get("steam_appid"):
            self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì„ ë°ì´í„° í˜•ì‹")
            return None
        
        header_image_url = data.get("header_image")
        uploaded_image_url = self.upload_image_to_s3(header_image_url, data["steam_appid"]) if header_image_url else None
            
        return {
            "appid": data.get("steam_appid"),
            "name": data.get("name"),
            "short_description": self.clean_html_entities(data.get("short_description")),
            "is_free": data.get("is_free", False),
            "release_date": data.get("release_date", {}).get("date", "ì •ë³´ ì—†ìŒ"),            
            "header_image": uploaded_image_url,
            "developer": data.get("developers", [None])[0],
            "publisher": data.get("publishers", [None])[0],
            "initial_price": int(data.get("price_overview", {}).get("initial", 0) / 100),
            "final_price": int(data.get("price_overview", {}).get("final", 0) / 100),
            "discount_percent": data.get("price_overview", {}).get("discount_percent", 0),
            "categories": [c["description"] for c in data.get("categories", [])],
            "genres": [g["description"] for g in data.get("genres", [])],
        }
    
    def fetch_detail_data(self, app_id):
        """Steam APIì—ì„œ ìƒì„¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        cached = self.cache.get(app_id)
        
        # ìºì‹œëœ ë°ì´í„°ê°€ ì„±ê³µì ì´ê³  48ì‹œê°„ ì´ë‚´ì— ìˆ˜ì§‘ëœ ê²½ìš°
        if cached and cached.get("status") == "success" and \
                not self.cache.is_stale(app_id, hours=48):
            return False
        
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        if self.cache.too_many_fails(app_id):
            self.logger.info(f"ğŸš« ì•± {app_id}ì€ ì‹¤íŒ¨ê°€ ëˆ„ì ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        
        url = f"https://store.steampowered.com/api/appdetails?appids={app_id}&l=korean"
        try:
            response = requests.get(url, headers=self.HEADERS, timeout=10)
            response.raise_for_status()
            response_json = response.json()
            app_data = response_json.get(str(app_id), {})

            if not app_data.get("success", False):
                self.failed_list.append(app_id)
                self.cache.record_fail(app_id)
                return False
            
            data = app_data.get("data", {})
            if data.get("type") == "game":
                self.original_data[app_id] = data
                parsed = self.parse_game_data(data)
                if parsed:
                    self.parsed_data.append(parsed)
                self.cache.set(app_id, {
                    "status": "success",
                    "collected_at": datetime.now().isoformat()
                })
                return True
            else:
                self.failed_list.append(app_id)
                self.cache.record_fail(app_id)
                self.logger.warning(f"[{app_id}] ê²Œì„ì´ ì•„ë‹™ë‹ˆë‹¤.")
                return False
                
        except Timeout:
            self.errored_list.append(app_id)
            self.cache.record_fail(app_id)
            self.logger.warning(f"[{app_id}] ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            raise
            
        except ConnectionError as e:
            self.errored_list.append(app_id)
            self.cache.record_fail(app_id)
            self.logger.error(f"[{app_id}] ì—°ê²° ì˜¤ë¥˜: {e}")
            raise
            
        except HTTPError as e:
            self.errored_list.append(app_id)
            status_code = getattr(e.response, 'status_code', None)
            self.cache.record_fail(app_id)
            
            if status_code == 429:  # Rate limit
                self.logger.warning(f"[{app_id}] ìš”ì²­ ì œí•œ ê°ì§€")
                self.rate_limit_manager.handle_rate_limit(app_id)
            raise
                
        except Exception as e:
            self.errored_list.append(app_id)
            self.cache.record_fail(app_id)
            self.logger.error(f"[{app_id}] ì—ëŸ¬ ë°œìƒ: {e}")
            raise
        
    def save_checkpoint(self):
        """í˜„ì¬ ìˆ˜ì§‘ ìƒíƒœ ì €ì¥"""
        try:
            # ì›ë³¸ ë°ì´í„° ì €ì¥
            new_original_df = pd.DataFrame.from_dict(self.original_data, orient="index")
            
            # ê¸°ì¡´ CSV ë¡œë“œ ë° ë³‘í•©
            if self.original_df_path.exists():
                old_original_df = load_csv(self.original_df_path)
                merged_original_df = pd.concat([old_original_df, new_original_df], ignore_index=True)
                merged_original_df.drop_duplicates(subset="steam_appid", inplace=True)
            else:
                merged_original_df = new_original_df
                
            # íŒŒì‹± ë°ì´í„° ì €ì¥
            new_parsed_df = pd.DataFrame(self.parsed_data)
            
            if self.parsed_df_path.exists():
                old_parsed_df = load_csv(self.parsed_df_path)
                merged_parsed_df = pd.concat([old_parsed_df, new_parsed_df], ignore_index=True)
                merged_parsed_df.drop_duplicates(subset="appid", inplace=True)
            else:
                merged_parsed_df = new_parsed_df
            
            # ì €ì¥
            save_csv(merged_original_df, self.original_df_path)
            save_csv(merged_parsed_df, self.parsed_df_path)
            self.cache.save()
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            total = len(self.parsed_data) + len(self.failed_list) + len(self.errored_list)
            success_rate = len(self.parsed_data) / total * 100 if total > 0 else 0
            print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ - ëˆ„ì  ìˆ˜ì§‘: {len(merged_parsed_df)}ê°œ")
            print(f"ì§„í–‰ ìƒí™©: {len(self.parsed_data)}ê°œ ì„±ê³µ / {len(self.failed_list)}ê°œ ì‹¤íŒ¨ / {len(self.errored_list)}ê°œ ì˜¤ë¥˜ (ì„±ê³µë¥ : {success_rate:.1f}%)")
            
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def fetch_in_parallel(self, app_ids, batch_size=40):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        base_delay = 1
        
        for i in range(0, len(app_ids), batch_size):
            batch = app_ids[i:i+batch_size]
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{i+len(batch)}/{len(app_ids)}")
            
            # ìš”ì²­ ì œí•œ ìƒí™©ì— ë”°ë¼ ì§€ì—° ì‹œê°„ ì¡°ì •
            request_delay = self.rate_limit_manager.get_current_delay(base_delay)
            if self.rate_limit_manager.should_slow_down():
                self.logger.info(f"âš ï¸ ìš”ì²­ ì œí•œ ê°ì§€ë¡œ ì§€ì—° ì‹œê°„ ì¦ê°€: {request_delay:.2f}ì´ˆ")
            
            for app_id in tqdm(batch, desc="ğŸ“¦ Fetching"):
                try:
                    should_sleep = self.fetch_detail_data(app_id)
                    if should_sleep:
                        time.sleep(request_delay)
                except Exception as e:
                    self.logger.error(f"[{app_id}] ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê° ë°°ì¹˜ í›„ ì €ì¥
            self.save_checkpoint()
    
    def retry_loop(self, target_list, label, max_retries=2):
        """ì‹¤íŒ¨í•œ ìš”ì²­ ì¬ì‹œë„"""
        for i in range(max_retries):
            if not target_list:
                break
                
            self.logger.info(f"ğŸ” {label} ì¬ì‹œë„ {i+1}íšŒ - ëŒ€ìƒ {len(target_list)}ê°œ")
            
            retry_targets = list(set(target_list))
            target_list.clear()
            
            # ì¬ì‹œë„ëŠ” ë” ì ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬
            self.fetch_in_parallel(retry_targets, batch_size=20)
            time.sleep(2)  # ì¬ì‹œë„ ì‚¬ì´ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
        
    def get_collected_appids(self):
        """ì´ë¯¸ ìˆ˜ì§‘ëœ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        collected_appids = set()
        if self.parsed_df_path.exists():
            try:
                existing_df = load_csv(self.parsed_df_path)
                collected_appids.update(existing_df["appid"].tolist())
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return collected_appids
    
    def run(self, input_csv_path):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        # ID ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ë° í•„í„°ë§
        ids_list = load_csv(input_csv_path)["appid"].tolist()
        # ids_list = [appid for appid in ids_list if appid not in collected_appids]
        self.logger.info(f"ğŸ“‹ ìƒˆë¡œ ìˆ˜ì§‘í•  appid ìˆ˜: {len(ids_list)}")
        
        # 1ì°¨ ìˆ˜ì§‘
        self.fetch_in_parallel(ids_list)
        
        # ì‹¤íŒ¨í•œ ID ê°€ì ¸ì˜¤ê¸°
        failed_ids_from_cache = [
            int(app_id) for app_id, status in self.cache.items()
            if status == "failed"
        ]
        self.logger.info(f"ìºì‹œì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_cache)}")

        # ì‹¤íŒ¨í•œ ID íŒŒì¼ì—ì„œ ì¶”ê°€ ì‹¤íŒ¨ ID ê°€ì ¸ì˜¤ê¸°
        if self.FAILED_IDS_FILE.exists():
            failed_ids_from_file = load_csv(self.FAILED_IDS_FILE)["appid"].tolist()
        else:
            failed_ids_from_file = []
        self.logger.info(f"íŒŒì¼ì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_file)}")
        
        # ì‹¤íŒ¨í•œ ID í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
        retry_ids = set(failed_ids_from_cache + failed_ids_from_file)
        
        # ì´ë¯¸ ì„±ê³µí•œ IDëŠ” ì œì™¸
        retry_ids = [appid for appid in retry_ids if self.cache.get(appid)["status"] != "success"]
        retry_ids = list(set(retry_ids))
        self.logger.info(f"ì¬ì‹œë„í•  ID ìˆ˜: {len(retry_ids)}")
        
        # ì¬ì‹œë„ ë£¨í”„
        retry_stages = [
            {"label": "ì—ëŸ¬", "targets": self.errored_list, "max_retries": 3},
            {"label": "ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 3},
            {"label": "ìµœì¢… ì—ëŸ¬", "targets": self.errored_list, "max_retries": 1},
            {"label": "ìµœì¢… ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 1},
            {"label": "ë§ˆì§€ë§‰", "targets": retry_ids, "max_retries": 1},
        ]
        
        for stage in retry_stages:
            target_list = stage["targets"]
            label = stage["label"]
            max_retries = stage["max_retries"]
            
            # ì¤‘ë³µ ì œê±°
            target_list[:] = list(set(target_list))
            
            if target_list:
                self.retry_loop(target_list, label, max_retries)
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        self.logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        
        if self.parsed_df_path.exists():
            # ìˆ˜ì§‘ ì™„ë£Œ í›„ ì •ì œ
            parsed_df = load_csv(self.parsed_df_path)
            common_ids = load_csv(input_csv_path)["appid"].tolist()
            filtered_df = parsed_df[parsed_df["appid"].isin(common_ids)]

            # ì´ ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
            self.logger.info(f"ğŸ¯ ì´ ìˆ˜ì§‘ëœ ê²Œì„ ìˆ˜: {len(filtered_df)}")
            save_csv(filtered_df, self.parsed_df_path)
        
        self.logger.info(f"âŒ ìµœì¢… ì‹¤íŒ¨: {len(self.failed_list)}ê°œ, ì—ëŸ¬: {len(self.errored_list)}ê°œ")
        
        # ì‹¤íŒ¨í•œ ID ëª©ë¡ ì €ì¥
        if self.failed_list or self.errored_list:
            failed_df = pd.DataFrame({
                "appid": self.failed_list + self.errored_list,
                "status": ["failed"] * len(self.failed_list) + ["error"] * len(self.errored_list)
            })
            failed_df.to_csv(self.ERROR_DIR / "failed_detail_ids.csv", index=False)
            self.logger.info(f"â— ì‹¤íŒ¨í•œ ID ëª©ë¡ì´ failed_detail_ids.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        # S3ì— ì—…ë¡œë“œ
        upload_to_s3(self.parsed_df_path, self.S3_OUTPUT_PARSED_KEY)
        upload_to_s3(self.original_df_path, self.S3_OUTPUT_ORIGINAL_KEY)
        upload_to_s3(self.CACHE_FILE, self.S3_CACHE_KEY)
        self.logger.info(f"âœ… S3ì— ì—…ë¡œë“œ ì™„ë£Œ: {self.S3_OUTPUT_PARSED_KEY}, {self.S3_CACHE_KEY}")
