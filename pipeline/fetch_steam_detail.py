import requests
import time
import html
from datetime import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path
from requests.exceptions import HTTPError, Timeout, ConnectionError

from util.io_helper import load_json, save_json, save_csv  
from util.logger import setup_logger
from util.rate_limit_manager import RateLimitManager

class SteamDetailFetcher:
    def __init__(self, 
                 output_dir="./data/raw", 
                 cache_dir="./data/cache",
                 error_dir="./data/error",
                 log_dir='./log/fetcher',
                 max_retries=3, 
                 thread_workers=2):
        # ------------------------- ì„¤ì • -------------------------
        self.MAX_RETRIES = max_retries
        self.THREAD_WORKERS = thread_workers
        self.OUTPUT_DIR = Path(output_dir)
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR = Path(cache_dir)
        self.CACHE_DIR.mkdir(exist_ok=True)
        self.ERROR_DIR = Path(error_dir)
        self.ERROR_DIR.mkdir(exist_ok=True)
        self.LOG_DIR = Path(log_dir)
        self.LOG_DIR.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = setup_logger(
            name="steam_detail_fetcher", 
            log_dir=self.LOG_DIR,
        )
        
        # ìš”ì²­ ì œì–´ê¸° ì„¤ì •
        self.rate_limit_manager = RateLimitManager()
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.CACHE_FILE = self.CACHE_DIR / "detail_status_cache.json"
        self.FAILED_IDS_FILE = self.ERROR_DIR / "failed_detail_ids.csv"
        self.original_df_path = self.OUTPUT_DIR / "steam_game_detail_original.csv"
        self.parsed_df_path = self.OUTPUT_DIR / "steam_game_detail_parsed.csv"
        
        # í—¤ë” ì„¤ì •
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        # ------------------------- ë°ì´í„° ì €ì¥ì†Œ -------------------------
        self.original_data = {}
        self.parsed_data = []
        self.failed_list = []
        self.errored_list = []
        self.status_cache = self._load_cache()
        
    def _load_cache(self):
        """ìºì‹œ íŒŒì¼ì—ì„œ ìƒíƒœ ì •ë³´ ë¡œë“œ"""
        try:
            cache = load_json(self.CACHE_FILE)
            if not isinstance(cache, dict):
                self.logger.warning("âš ï¸ ìºì‹œ íŒŒì¼ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                return {}
            # ì¤‘ë³µ í‚¤ ì œê±° (ë§ˆì§€ë§‰ ê°’ ìœ ì§€)
            deduplicated_cache = {}
            for key, value in cache.items():
                deduplicated_cache[key] = value
            
            # ë¡œê¹… ì¶”ê°€: ì¤‘ë³µ ì œê±°ëœ í‚¤ì˜ ìˆ˜ í™•ì¸
            original_count = len(cache)
            deduplicated_count = len(deduplicated_cache)
            
            if original_count != deduplicated_count:
                self.logger.info(f"ğŸ” ìºì‹œì—ì„œ {original_count - deduplicated_count}ê°œì˜ ì¤‘ë³µ í‚¤ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return deduplicated_cache
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            return {}
        
    def clean_html_entities(self, text):
        if pd.isna(text):
            return text
        return html.unescape(text)    
        
    def parse_game_data(self, data):
        """API ì‘ë‹µì—ì„œ í•„ìš”í•œ ê²Œì„ ì •ë³´ ì¶”ì¶œ"""
        if not isinstance(data, dict) or not data.get("steam_appid"):
            self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì„ ë°ì´í„° í˜•ì‹")
            return None
            
        return {
            "appid": data.get("steam_appid"),
            "name": data.get("name"),
            "short_description": self.clean_html_entities(data.get("short_description")),
            "is_free": data.get("is_free", False),
            "release_date": data.get("release_date", {}).get("date", "ì •ë³´ ì—†ìŒ"),            
            "header_image": data.get("header_image"),
            "developer": data.get("developers", [None])[0],
            "publisher": data.get("publishers", [None])[0],
            "initial_price": int(data.get("price_overview", {}).get("initial", 0) / 100),
            "final_price": int(data.get("price_overview", {}).get("final", 0) / 100),
            "discount_percent": data.get("price_overview", {}).get("discount_percent", 0),
            "categories": [c["description"] for c in data.get("categories", [])],
            "genres": [g["description"] for g in data.get("genres", [])],
        }
    
    def fetch_detail_data(self, app_id):
        """Steam APIì—ì„œ ìƒì„¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if str(app_id) in self.status_cache and self.status_cache[str(app_id)] == "success":
            return
        
        url = f"https://store.steampowered.com/api/appdetails?appids={app_id}&l=korean"
        try:
            response = requests.get(url, headers=self.HEADERS, timeout=10)
            response.raise_for_status()
            response_json = response.json()
            app_data = response_json.get(str(app_id), {})

            if not app_data.get("success", False):
                self.failed_list.append(app_id)
                self.status_cache[str(app_id)] = "failed"
                return
            
            data = app_data.get("data", {})
            if data.get("type") == "game":
                self.original_data[app_id] = data
                parsed = self.parse_game_data(data)
                if parsed:
                    self.parsed_data.append(parsed)
                self.status_cache[app_id] = {
                    "status": "success",
                    "collected_at": datetime.now().isoformat()
                }
            else:
                self.failed_list.append(app_id)
                self.status_cache[str(app_id)] = "not_game"
                
        except Timeout:
            self.errored_list.append(app_id)
            self.status_cache[app_id] = "timeout"
            self.logger.warning(f"[{app_id}] ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            raise
            
        except ConnectionError as e:
            self.errored_list.append(app_id)
            self.status_cache[app_id] = "connection_error"
            self.logger.error(f"[{app_id}] ì—°ê²° ì˜¤ë¥˜: {e}")
            raise
            
        except HTTPError as e:
            self.errored_list.append(app_id)
            status_code = getattr(e.response, 'status_code', None)
            self.status_cache[app_id] = f"http_error_{status_code}"
            
            if status_code == 429:  # Rate limit
                self.logger.warning(f"[{app_id}] ìš”ì²­ ì œí•œ ê°ì§€")
                self.rate_limit_manager.handle_rate_limit(app_id)
            raise
                
        except Exception as e:
            self.errored_list.append(app_id)
            self.status_cache[app_id] = "error"
            self.logger.error(f"[{app_id}] ì—ëŸ¬ ë°œìƒ: {e}")
            raise
        
    def save_checkpoint(self):
        """í˜„ì¬ ìˆ˜ì§‘ ìƒíƒœ ì €ì¥"""
        try:
            # ì›ë³¸ ë°ì´í„° ì €ì¥
            new_original_df = pd.DataFrame.from_dict(self.original_data, orient="index")
            
            # ê¸°ì¡´ CSV ë¡œë“œ ë° ë³‘í•©
            if self.original_df_path.exists():
                old_original_df = pd.read_csv(self.original_df_path)
                merged_original_df = pd.concat([old_original_df, new_original_df], ignore_index=True)
                merged_original_df.drop_duplicates(subset="steam_appid", inplace=True)
            else:
                merged_original_df = new_original_df
                
            # íŒŒì‹± ë°ì´í„° ì €ì¥
            new_parsed_df = pd.DataFrame(self.parsed_data)
            
            if self.parsed_df_path.exists():
                old_parsed_df = pd.read_csv(self.parsed_df_path)
                merged_parsed_df = pd.concat([old_parsed_df, new_parsed_df], ignore_index=True)
                merged_parsed_df.drop_duplicates(subset="appid", inplace=True)
            else:
                merged_parsed_df = new_parsed_df
            
            # ìƒíƒœ ìºì‹œì˜ ì¤‘ë³µ ì œê±°
            clean_status_cache = {}
            for key, value in self.status_cache.items():
                clean_status_cache[key] = value
            
            # ì €ì¥
            save_csv(merged_original_df, self.original_df_path)
            save_csv(merged_parsed_df, self.parsed_df_path)
            save_json(self.CACHE_FILE, clean_status_cache)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            total = len(self.parsed_data) + len(self.failed_list) + len(self.errored_list)
            success_rate = len(self.parsed_data) / total * 100 if total > 0 else 0
            print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ - ëˆ„ì  ìˆ˜ì§‘: {len(merged_parsed_df)}ê°œ")
            print(f"ì§„í–‰ ìƒí™©: {len(self.parsed_data)}ê°œ ì„±ê³µ / {len(self.failed_list)}ê°œ ì‹¤íŒ¨ / {len(self.errored_list)}ê°œ ì˜¤ë¥˜ (ì„±ê³µë¥ : {success_rate:.1f}%)")
            
            # ì €ì¥ í›„ ë°ì´í„° ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            # self.original_data = {}
            # self.parsed_data = []
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def fetch_in_parallel(self, app_ids, batch_size=40):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì²˜ë¦¬"""
        base_delay = 0.8
        
        for i in range(0, len(app_ids), batch_size):
            batch = app_ids[i:i+batch_size]
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{i+len(batch)}/{len(app_ids)}")
            
            # ìš”ì²­ ì œí•œ ìƒí™©ì— ë”°ë¼ ì§€ì—° ì‹œê°„ ì¡°ì •
            request_delay = self.rate_limit_manager.get_current_delay(base_delay)
            if self.rate_limit_manager.should_slow_down():
                self.logger.info(f"âš ï¸ ìš”ì²­ ì œí•œ ê°ì§€ë¡œ ì§€ì—° ì‹œê°„ ì¦ê°€: {request_delay:.2f}ì´ˆ")
            
            with ThreadPoolExecutor(max_workers=self.THREAD_WORKERS) as executor:
                # ìš”ì²­ ì œì¶œ ì‹œ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€
                futures = []
                for app_id in batch:
                    futures.append(executor.submit(self.fetch_detail_data, app_id))
                    time.sleep(request_delay)  # ìš”ì²­ ê°„ 800ms ì§€ì—°
                
                for future in tqdm(as_completed(futures), total=len(batch), desc="ğŸ“¦ Fetching"):
                    try:
                        future.result()
                    except Exception as e:
                        self.logger.error(f"ìŠ¤ë ˆë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê° ë°°ì¹˜ í›„ ì €ì¥
            self.save_checkpoint()
    
    def retry_loop(self, target_list, label, max_retries=3):
        """ì‹¤íŒ¨í•œ ìš”ì²­ ì¬ì‹œë„"""
        for i in range(max_retries):
            if not target_list:
                break
                
            self.logger.info(f"ğŸ” {label} ì¬ì‹œë„ {i+1}íšŒ - ëŒ€ìƒ {len(target_list)}ê°œ")
            
            retry_targets = list(set(target_list))
            target_list.clear()
            
            # ì¬ì‹œë„ëŠ” ë” ì ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬
            self.fetch_in_parallel(retry_targets, batch_size=20)
            time.sleep(2)  # ì¬ì‹œë„ ì‚¬ì´ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
        
    def get_collected_appids(self):
        """ì´ë¯¸ ìˆ˜ì§‘ëœ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        collected_appids = set()
        if self.parsed_df_path.exists():
            try:
                existing_df = pd.read_csv(self.parsed_df_path)
                collected_appids.update(existing_df["appid"].tolist())
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return collected_appids
    
    def run(self, input_csv_path):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        # ì´ë¯¸ ìˆ˜ì§‘ëœ ID í™•ì¸
        collected_appids = self.get_collected_appids()
        
        # ID ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ë° í•„í„°ë§
        ids_list = pd.read_csv(input_csv_path)["appid"].tolist()
        ids_list = [appid for appid in ids_list if appid not in collected_appids]
        self.logger.info(f"ğŸ“‹ ìƒˆë¡œ ìˆ˜ì§‘í•  appid ìˆ˜: {len(ids_list)}")
        
        # 1ì°¨ ìˆ˜ì§‘
        self.fetch_in_parallel(ids_list)
        
        # ì‹¤íŒ¨í•œ ID ê°€ì ¸ì˜¤ê¸°
        failed_ids_from_cache = [
            int(app_id) for app_id, status in self.status_cache.items()
            if status == "failed"
        ]
        self.logger.info(f"ìºì‹œì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_cache)}")

        # ì‹¤íŒ¨í•œ ID íŒŒì¼ì—ì„œ ì¶”ê°€ ì‹¤íŒ¨ ID ê°€ì ¸ì˜¤ê¸°
        if self.FAILED_IDS_FILE.exists():
            failed_ids_from_file = pd.read_csv(self.FAILED_IDS_FILE)["appid"].tolist()
        else:
            failed_ids_from_file = []
        self.logger.info(f"íŒŒì¼ì—ì„œ ì‹¤íŒ¨í•œ ID ìˆ˜: {len(failed_ids_from_file)}")
        
        # ì‹¤íŒ¨í•œ ID í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
        retry_ids = set(failed_ids_from_cache + failed_ids_from_file)
        
        # ì´ë¯¸ ì„±ê³µí•œ IDëŠ” ì œì™¸
        retry_ids = [appid for appid in retry_ids if self.status_cache.get(appid) != "success"]
        retry_ids = list(set(retry_ids))
        self.logger.info(f"ì¬ì‹œë„í•  ID ìˆ˜: {len(retry_ids)}")
        
        # ì¬ì‹œë„ ë£¨í”„
        retry_stages = [
            {"label": "ì—ëŸ¬", "targets": self.errored_list, "max_retries": 3},
            {"label": "ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 3},
            {"label": "ìµœì¢… ì—ëŸ¬", "targets": self.errored_list, "max_retries": 1},
            {"label": "ìµœì¢… ì‹¤íŒ¨", "targets": self.failed_list, "max_retries": 1},
            {"label": "ë§ˆì§€ë§‰", "targets": retry_ids, "max_retries": 1},
        ]
        
        for stage in retry_stages:
            target_list = stage["targets"]
            label = stage["label"]
            max_retries = stage["max_retries"]
            
            # ì¤‘ë³µ ì œê±°
            target_list[:] = list(set(target_list))
            
            if target_list:
                self.retry_loop(target_list, label, max_retries)
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        self.logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì´ ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
        if self.parsed_df_path.exists():
            final_df = pd.read_csv(self.parsed_df_path)
            self.logger.info(f"ğŸ¯ ì´ ìˆ˜ì§‘ëœ ê²Œì„ ë¦¬ë·° ìˆ˜: {len(final_df)}")
        
        self.logger.info(f"âŒ ìµœì¢… ì‹¤íŒ¨: {len(self.failed_list)}ê°œ, ì—ëŸ¬: {len(self.errored_list)}ê°œ")
        
        # ì‹¤íŒ¨í•œ ID ëª©ë¡ ì €ì¥
        if self.failed_list or self.errored_list:
            failed_df = pd.DataFrame({
                "appid": self.failed_list + self.errored_list,
                "status": ["failed"] * len(self.failed_list) + ["error"] * len(self.errored_list)
            })
            failed_df.to_csv(self.ERROR_DIR / "failed_detail_ids.csv", index=False)
            self.logger.info(f"â— ì‹¤íŒ¨í•œ ID ëª©ë¡ì´ failed_detail_ids.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
