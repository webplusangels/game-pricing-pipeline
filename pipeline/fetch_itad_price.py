import requests
import time
import pandas as pd
from tqdm import tqdm
from pathlib import Path
from requests.exceptions import HTTPError, Timeout, ConnectionError
from datetime import datetime
from config import settings

from util.io_helper import save_csv, load_csv, upload_to_s3, download_from_s3
from util.cache_manager import CacheManager
from util.logger import setup_logger
from util.rate_limit_manager import RateLimitManager

class ITADPriceFetcher:
    def __init__(self, 
                 output_dir="./data/raw", 
                 cache_dir="./data/cache",
                 error_dir="./data/error",
                 log_dir='./log/fetcher',
                 max_retries=3, 
                 thread_workers=2):
        # ------------------------- ì„¤ì • -------------------------
        self.MAX_RETRIES = max_retries
        self.THREAD_WORKERS = thread_workers
        self.OUTPUT_DIR = Path(output_dir)
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR = Path(cache_dir)
        self.CACHE_DIR.mkdir(exist_ok=True)
        self.ERROR_DIR = Path(error_dir)
        self.ERROR_DIR.mkdir(exist_ok=True)
        self.LOG_DIR = Path(log_dir)
        self.LOG_DIR.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = setup_logger(
            name="itad_price_fetcher", 
            log_dir=self.LOG_DIR,
        )
        
        # ìš”ì²­ ì œì–´ê¸° ì„¤ì •
        self.rate_limit_manager = RateLimitManager()
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.CACHE_FILE = self.CACHE_DIR / "itad_price_status_cache.json"
        self.FAILED_IDS_FILE = self.ERROR_DIR / "itad_price_failed_ids.csv"
        self.INPUT_FILE = self.OUTPUT_DIR / "itad_game_ids.csv"
        self.OUTPUT_FILE = self.OUTPUT_DIR / "itad_game_prices.csv"

        # API ì„¤ì •
        self.PRICE_BASE_URL = "https://api.isthereanydeal.com/games/prices/v3"
        self.KEY = settings.ITAD_KEY
        self.COUNTRY = "KR"
                
        # S3 ì—…ë¡œë“œ ì„¤ì •
        self.S3_CACHE_KEY = "data/cache/itad_price_status_cache.json"
        self.S3_OUTPUT_KEY = "data/raw/itad_game_prices.csv"
        if self.CACHE_FILE.exists():
            self.logger.info("ğŸ“ ë¡œì»¬ ìºì‹œ ì‚¬ìš©")
        elif download_from_s3(self.S3_CACHE_KEY, self.CACHE_FILE):
            self.logger.info("âœ… S3 ìºì‹œ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
        else:
            self.logger.warning("â— ìºì‹œ íŒŒì¼ ì—†ìŒ. ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")

        # í—¤ë” ì„¤ì •
        self.HEADERS = {
            "Content-Type": "application/json",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        # ------------------------- ë°ì´í„° ì €ì¥ì†Œ -------------------------
        self.fetched_data = []
        self.failed_list = []
        self.errored_list = []
        self.cache = CacheManager(self.CACHE_FILE)
    
    def load_game_ids(self):
        """ê²Œì„ ID íŒŒì¼ì—ì„œ ê²Œì„ ID ëª©ë¡ ë¡œë“œ"""
        try:
            if not self.INPUT_FILE.exists():
                raise FileNotFoundError(f"{self.INPUT_FILE} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            df = load_csv(self.INPUT_FILE)
            if 'itad_id' not in df.columns:
                raise ValueError("ì…ë ¥ CSVì— 'itad_id' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            valid_ids = df.dropna(subset=['itad_id'])

            # # ì´ë¯¸ ì²˜ë¦¬ëœ ID ì œì™¸
            # if self.OUTPUT_FILE.exists():
            #     existing_df = pd.read_csv(self.OUTPUT_FILE)
            #     if 'itad_id' in existing_df.columns:
            #         processed_ids = set(existing_df['itad_id'].tolist())
            #         valid_ids = valid_ids[~valid_ids['itad_id'].isin(processed_ids)]

            self.logger.info(f"ì´ {len(valid_ids)}ê°œì˜ ìœ íš¨í•œ ê²Œì„ IDë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return valid_ids

        except Exception as e:
            self.logger.error(f"ê²Œì„ ID ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return pd.DataFrame()
    
    def get_game_prices(self, game_ids_batch):
        """ê²Œì„ ê°€ê²© ì •ë³´ API ìš”ì²­"""
        url = f"{self.PRICE_BASE_URL}?key={self.KEY}&country={self.COUNTRY}"
        try:
            response = requests.post(
                url,
                headers=self.HEADERS,
                json=game_ids_batch,
                timeout=15
            )
            response.raise_for_status()
            data = response.json()
            
            if isinstance(data, list):
                processed_data = {}
                for entry in data:
                    if 'id' in entry:
                        processed_data[entry['id']] = entry
                return processed_data            
            return data
        except Exception as e:
            self.logger.error(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def process_price_data(self, data, game_ids_batch):
        """API ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬"""
        results = []
        processed_ids = set()

        if data is None:
            self.logger.warning("APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            self.failed_list.extend(game_ids_batch)
            return results

        for game_id in game_ids_batch:
            if game_id in processed_ids:
                continue

            processed_ids.add(game_id)

            cached = self.cache.get(game_id)

            if cached and cached.get("status") == "success" and \
            not self.cache.is_stale(game_id, hours=3):
                self.logger.info(f"[{game_id}] ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°, ê±´ë„ˆëœ€")
                continue

            if self.cache.too_many_fails(game_id):
                self.logger.info(f"ğŸš« ì•± {game_id}ì€ ì‹¤íŒ¨ê°€ ëˆ„ì ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            if game_id not in data:
                self.logger.warning(f"[{game_id}] API ì‘ë‹µì— í•´ë‹¹ ê²Œì„ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
                self.failed_list.append(game_id)
                continue

            game_data = data[game_id]

            try:
                # ì—­ëŒ€ ìµœì €ê°€ ì •ë³´ ì¶”ì¶œ
                history_low_price = None
                history_low_currency = None
                if 'historyLow' in game_data and 'all' in game_data['historyLow']:
                    history_low_price = game_data['historyLow']['all'].get('amount')
                    history_low_currency = game_data['historyLow']['all'].get('currency')
                    self.logger.debug(f"[{game_id}] ì—­ëŒ€ ìµœì €ê°€: {history_low_price} {history_low_currency}")

                # ë”œ ì •ë³´ ë¡œê¹…
                if 'deals' not in game_data:
                    self.logger.warning(f"[{game_id}] 'deals' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {game_data}")
                elif not game_data['deals']:
                    self.logger.info(f"[{game_id}] ë”œ ì •ë³´ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                else:
                    self.logger.info(f"[{game_id}] ë”œ ìˆ˜: {len(game_data['deals'])}")

                # ê° ë”œ ì •ë³´ ì²˜ë¦¬
                for deal in game_data['deals']:
                    result = {
                        'itad_id': game_id,
                        'history_low_price': history_low_price,
                        'history_low_currency': history_low_currency,
                        'shop_id': deal.get('shop', {}).get('id'),
                        'shop_name': deal.get('shop', {}).get('name'),
                        'current_price': deal.get('price', {}).get('amount'),
                        'regular_price': deal.get('regular', {}).get('amount'),
                        'url': deal.get('url', {}),
                        'discount_percent': deal.get('cut', 0),
                        'collected_at': datetime.now().isoformat()
                    }
                    results.append(result)

                self.cache.set(game_id, {
                    "status": "success",
                    "collected_at": datetime.now().isoformat()
                })
                self.logger.info(f"[{game_id}] ì²˜ë¦¬ ì™„ë£Œ: {len(game_data.get('deals', []))}ê°œì˜ ë”œ ì •ë³´")

            except Exception as e:
                self.logger.error(f"[{game_id}] ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                self.errored_list.append(game_id)
                self.cache.record_fail(game_id)
                
        return results
    
    def fetch_batch(self, game_ids_batch):
        """ë°°ì¹˜ ë‹¨ìœ„ ê²Œì„ ê°€ê²© ì •ë³´ ìˆ˜ì§‘"""
        retry_count = 0
        max_retries = self.MAX_RETRIES

        while retry_count < max_retries:
            try:
                self.logger.info(f"ë°°ì¹˜ ìš”ì²­ ì‹œì‘: {len(game_ids_batch)}ê°œ ê²Œì„")
                
                # ìš”ì²­ ì œí•œ ìƒí™©ì— ë”°ë¼ ì§€ì—° ì‹œê°„ ì¡°ì •
                if self.rate_limit_manager.should_slow_down():
                    delay = self.rate_limit_manager.get_current_delay(1.0)
                    self.logger.info(f"âš ï¸ ìš”ì²­ ì œí•œ ê°ì§€ë¡œ ì§€ì—° ì‹œê°„ ì¦ê°€: {delay:.2f}ì´ˆ")
                    time.sleep(delay)
                
                data = self.get_game_prices(game_ids_batch)
                processed_data = self.process_price_data(data, game_ids_batch)
                if processed_data:
                    self.fetched_data.extend(processed_data)
                    return True
                return False
                
            except Timeout:
                retry_count += 1
                self.logger.warning(f"ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì¬ì‹œë„ {retry_count}/{max_retries})")
                time.sleep(3 * retry_count)
                
            except ConnectionError as e:
                retry_count += 1
                self.logger.error(f"ì—°ê²° ì˜¤ë¥˜: {e} (ì¬ì‹œë„ {retry_count}/{max_retries})")
                time.sleep(5 * retry_count)
                
            except HTTPError as e:
                status_code = getattr(e.response, 'status_code', None)
                if status_code == 429:  # Rate limit
                    retry_count += 1
                    self.rate_limit_manager.handle_rate_limit()
                    wait_time = 10 * retry_count
                    self.logger.warning(f"ìš”ì²­ ì œí•œ ê°ì§€. {wait_time}ì´ˆ ëŒ€ê¸° (ì¬ì‹œë„ {retry_count}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    self.errored_list.extend(game_ids_batch)
                    self.logger.error(f"HTTP ì˜¤ë¥˜ {status_code}: {e}")
                    return False
                    
            except Exception as e:
                self.errored_list.extend(game_ids_batch)
                self.logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return False

        if retry_count >= max_retries:
            self.errored_list.extend(game_ids_batch)
            self.logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨.")
            return False
            
    def save_checkpoint(self):
        """í˜„ì¬ ìˆ˜ì§‘ ìƒíƒœë¥¼ ì €ì¥"""
        try:
            if not self.fetched_data:
                self.logger.info("ì €ì¥í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ìƒˆ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            new_data_df = pd.DataFrame(self.fetched_data)
    
            if self.OUTPUT_FILE.exists():
                old_data_df = load_csv(self.OUTPUT_FILE)
                merged_df = pd.concat([old_data_df, new_data_df], ignore_index=True)
                if 'shop_id' in merged_df.columns:
                    merged_df.drop_duplicates(subset=['itad_id', 'shop_id'], keep='last', inplace=True)
                else:
                    merged_df.drop_duplicates(subset=['itad_id'], keep='last', inplace=True)
            else:
                merged_df = new_data_df
    
            # ì €ì¥
            save_csv(merged_df, self.OUTPUT_FILE)
            self.cache.save()
    
            # ì •ë³´ ì¶œë ¥
            total_processed = len(set([d['itad_id'] for d in self.fetched_data]))
            self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ. ì´ {total_processed}ê°œ ê²Œì„, {len(self.fetched_data)}ê°œ ë”œ ì •ë³´ ì €ì¥")
    
            self.fetched_data = []

        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def fetch_in_parallel(self, game_ids, batch_size=40):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê²Œì„ ê°€ê²© ì •ë³´ ìˆ˜ì§‘"""
        batches = [game_ids[i:i+batch_size] for i in range(0, len(game_ids), batch_size)]
        
        with tqdm(total=len(batches), desc="ğŸ” ë°°ì¹˜ ì²˜ë¦¬") as pbar:
            for i, batch in enumerate(batches):
                should_sleep = self.fetch_batch(batch)
                self.save_checkpoint()
                pbar.update(1)
                
                if i < len(batches) - 1 and should_sleep:
                    time.sleep(2)
    
    def retry_failed_ids(self, batch_size=20, max_retry_rounds=2):
        """ì‹¤íŒ¨í•œ ìš”ì²­ ì¬ì‹œë„"""
        retry_ids = list(set(self.failed_list + self.errored_list))
        if not retry_ids:
            return
        
        self.logger.info(f"ì´ {len(retry_ids)}ê°œ ì‹¤íŒ¨ IDì— ëŒ€í•´ ì¬ì‹œë„ ì‹œì‘")

        for round_num in range(1, max_retry_rounds + 1):
            self.logger.info(f"ì¬ì‹œë„ ë¼ìš´ë“œ {round_num}/{max_retry_rounds}")
            self.failed_list = []
            self.errored_list = []
            batches = [retry_ids[i:i+batch_size] for i in range(0, len(retry_ids), batch_size)]
            
            with tqdm(total=len(batches), desc=f"â™»ï¸ ì¬ì‹œë„ {round_num}") as pbar:
                for batch in batches:
                    self.fetch_batch(batch)
                    self.save_checkpoint()
                    pbar.update(1)
                    time.sleep(2)
                    
            retry_ids = list(set(self.failed_list + self.errored_list))
            if not retry_ids:
                self.logger.info("âœ… ì¬ì‹œë„ ì„±ê³µ! ë” ì´ìƒ ì‹¤íŒ¨í•œ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
    def init_app_list(self, ids_list):
        """ìˆ˜ì§‘í•œ ID ëª©ë¡ ì´ˆê¸°í™”"""
        df = load_csv(self.players_df_path)
        if not df.empty:
            # ids_listì— ì—†ëŠ” ID ì œê±°
            df = df[~df["appid"].isin(ids_list)]
        save_csv(df, self.players_df_path)
        return len(df)
    
    def run(self, batch_size=40):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            game_ids_df = self.load_game_ids()
            if game_ids_df.empty:
                self.logger.warning("âŒ ì²˜ë¦¬í•  ê²Œì„ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            game_ids = game_ids_df['itad_id'].dropna().tolist()
            if not game_ids:
                self.logger.warning("âŒ ìœ íš¨í•œ itad_idê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            total_games = len(game_ids)
            self.logger.info(f"ğŸ“‹ ì´ {total_games}ê°œ ê²Œì„ ê°€ê²© ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")

            # 1ì°¨ ìˆ˜ì§‘
            self.fetch_in_parallel(game_ids, batch_size=batch_size)
            
            # ì‹¤íŒ¨í•œ ID ì¬ì‹œë„
            self.retry_failed_ids(batch_size=batch_size, max_retry_rounds=2)

            # ìµœì¢… í†µê³„
            total_success = sum(1 for status in self.cache.values() if status == "success")
            self.logger.info("\nâœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            self.logger.info(f"ğŸ¯ ì„±ê³µ: {total_success}ê°œ ê²Œì„")
            self.logger.info(f"âŒ ì‹¤íŒ¨: {len(self.failed_list)}ê°œ, ì—ëŸ¬: {len(self.errored_list)}ê°œ")
            
            # ì‹¤íŒ¨í•œ ID ëª©ë¡ ì €ì¥
            if self.failed_list or self.errored_list:
                failed_df = pd.DataFrame({
                    "itad_id": self.failed_list + self.errored_list,
                    "status": ["failed"] * len(self.failed_list) + ["error"] * len(self.errored_list)
                })
                save_csv(failed_df, self.FAILED_IDS_FILE)
                self.logger.info(f"â— ì‹¤íŒ¨í•œ ID ëª©ë¡ì´ {self.FAILED_IDS_FILE}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            if self.OUTPUT_FILE.exists():
                final_df = load_csv(self.OUTPUT_FILE)
                unique_games = final_df['itad_id'].nunique()
                total_deals = len(final_df)
                shops_count = final_df['shop_name'].value_counts().to_dict()
                self.logger.info(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
                self.logger.info(f"ì´ {unique_games}ê°œ ê²Œì„, {total_deals}ê°œ ë”œ ì •ë³´ ìˆ˜ì§‘")
                self.logger.info(f"ìƒì ë³„ ë¶„í¬: {shops_count}")
                
            # S3 ì—…ë¡œë“œ
            upload_to_s3(self.OUTPUT_FILE, self.S3_OUTPUT_KEY)
            upload_to_s3(self.CACHE_FILE, self.S3_CACHE_KEY)
            self.logger.info(f"âœ… S3ì— ì—…ë¡œë“œ ì™„ë£Œ: {self.S3_OUTPUT_KEY}, {self.S3_CACHE_KEY}")

        except Exception as e:
            self.logger.error(f"í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

# # ì‚¬ìš© ì˜ˆì‹œ
# if __name__ == "__main__":
#     fetcher = ITADPriceFetcher()
#     fetcher.run(batch_size=40)